{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Fine-tuning GPT-2 on a dataset in PyTorch\n\nFine-tune a pre-trained GPT-2 model on a dataset containing the scripts of a python course. Let's see if the model can learn to teach Python!\n\nFor this experiment, we will use a pre-trained GPT-2 medium-sized model from the huggingface [transformers repository](https://github.com/huggingface/transformers).\n\nThis project is developed referencing https://gist.github.com/mf1024/3df214d2f17f3dcc56450ddf0d5a4cd7\n","metadata":{}},{"cell_type":"code","source":"!pip install transformers","metadata":{"execution":{"iopub.status.busy":"2022-11-04T20:34:18.292700Z","iopub.execute_input":"2022-11-04T20:34:18.293371Z","iopub.status.idle":"2022-11-04T20:34:28.890826Z","shell.execute_reply.started":"2022-11-04T20:34:18.293337Z","shell.execute_reply":"2022-11-04T20:34:28.889585Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /opt/conda/lib/python3.7/site-packages (4.20.1)\nRequirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /opt/conda/lib/python3.7/site-packages (from transformers) (0.12.1)\nRequirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from transformers) (2.28.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /opt/conda/lib/python3.7/site-packages (from transformers) (0.10.1)\nRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from transformers) (4.13.0)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.7/site-packages (from transformers) (4.64.0)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.7/site-packages (from transformers) (1.21.6)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from transformers) (3.7.1)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.7/site-packages (from transformers) (2021.11.10)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.7/site-packages (from transformers) (6.0)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.7/site-packages (from transformers) (21.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.7/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.1.1)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging>=20.0->transformers) (3.0.9)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->transformers) (3.8.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (3.3)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (1.26.12)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (2022.9.24)\nRequirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (2.1.0)\n^C\n\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nfrom transformers import GPT2Tokenizer, GPT2LMHeadModel\nimport numpy as np\n\nimport logging\nlogging.getLogger().setLevel(logging.CRITICAL)\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\ndevice = 'cpu'\nif torch.cuda.is_available():\n    device = 'cuda'\nprint(device)","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2022-11-04T20:34:28.896405Z","iopub.execute_input":"2022-11-04T20:34:28.898634Z","iopub.status.idle":"2022-11-04T20:34:31.113701Z","shell.execute_reply.started":"2022-11-04T20:34:28.898593Z","shell.execute_reply":"2022-11-04T20:34:31.112650Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"cuda\n","output_type":"stream"}]},{"cell_type":"code","source":"tokenizer = GPT2Tokenizer.from_pretrained('gpt2-medium')\nmodel = GPT2LMHeadModel.from_pretrained('gpt2-medium')\nmodel = model.to(device)","metadata":{"execution":{"iopub.status.busy":"2022-11-04T20:34:31.115681Z","iopub.execute_input":"2022-11-04T20:34:31.116611Z","iopub.status.idle":"2022-11-04T20:35:15.402005Z","shell.execute_reply.started":"2022-11-04T20:34:31.116572Z","shell.execute_reply":"2022-11-04T20:35:15.401017Z"},"trusted":true},"execution_count":5,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/0.99M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a344a408ab65428a862dda66a41f6278"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/446k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2e985344d5a34e00b8dc565cc81247ff"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/718 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f717d1d95e7a4be690f333900d642994"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/1.42G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f9cd5b1d5b1f40bcab6a9c975bb7fc72"}},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### PyTorch Dataset module for the dataset\n\nWe will use the dataset of lecture scripts. After each paragraph, we add \"<|endofext|>\" which is recognized by the GPT2 model as the end of text marker. The marker will allow us to concatenate many paragraph in a single input sequence.","metadata":{}},{"cell_type":"code","source":"from torch.utils.data import Dataset\nfrom torch.utils.data import Dataset, DataLoader\nimport os\nimport json\nimport csv\n\nclass LecturesDataset(Dataset):\n    def __init__(self, jokes_dataset_path = '/kaggle/input/shortjokestest/'):\n        super().__init__()\n\n        short_jokes_path = os.path.join(jokes_dataset_path, 'shortjokes.csv')\n\n        self.joke_list = []\n        self.end_of_text_token = \"<|endoftext|>\"\n        \n        with open(short_jokes_path) as csv_file:\n            csv_reader = csv.reader(csv_file, delimiter=',')\n        \n            for row in csv_reader:\n                joke_str = f\"JOKE:{row[1]}{self.end_of_text_token}\"\n                self.joke_list.append(joke_str)\n        \n    def __len__(self):\n        return len(self.joke_list)\n\n    def __getitem__(self, item):\n        return self.joke_list[item]\n","metadata":{"execution":{"iopub.status.busy":"2022-11-04T20:35:15.404588Z","iopub.execute_input":"2022-11-04T20:35:15.406047Z","iopub.status.idle":"2022-11-04T20:35:15.414725Z","shell.execute_reply.started":"2022-11-04T20:35:15.406008Z","shell.execute_reply":"2022-11-04T20:35:15.413603Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"dataset = LecturesDataset()[:2000]\nlectures_loader = DataLoader(dataset, batch_size=1, shuffle=True)","metadata":{"execution":{"iopub.status.busy":"2022-11-04T20:35:15.416198Z","iopub.execute_input":"2022-11-04T20:35:15.416977Z","iopub.status.idle":"2022-11-04T20:35:15.933819Z","shell.execute_reply.started":"2022-11-04T20:35:15.416942Z","shell.execute_reply":"2022-11-04T20:35:15.932843Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"### Hyperparameters\n\nWe mostly tuned ***BATCH_SIZE***, ***EOPOCHS***, and ***LEARNING_RATE***.","metadata":{}},{"cell_type":"code","source":"BATCH_SIZE = 16\nEPOCHS = 5\nLEARNING_RATE = 3e-5\nWARMUP_STEPS = 500\nMAX_SEQ_LEN = 40","metadata":{"execution":{"iopub.status.busy":"2022-11-04T20:35:15.935426Z","iopub.execute_input":"2022-11-04T20:35:15.935817Z","iopub.status.idle":"2022-11-04T20:35:15.942655Z","shell.execute_reply.started":"2022-11-04T20:35:15.935760Z","shell.execute_reply":"2022-11-04T20:35:15.941153Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"### Model training\n\nWe will train the model and save the model weights after each epoch and then we will try to generate jokes with each version of the weight to see which performs the best.","metadata":{}},{"cell_type":"code","source":"from transformers import AdamW, get_linear_schedule_with_warmup\n\nmodel.train()\noptimizer = AdamW(model.parameters(), lr=LEARNING_RATE)\nscheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=WARMUP_STEPS, num_training_steps = -1)\nproc_seq_count = 0\nsum_loss = 0.0\nbatch_count = 0\n\ntmp_paragraphs_tens = None\nmodels_folder = \"/kaggle/working/trained_models\" #fold for the trained model\n\nif not os.path.exists(models_folder):\n    os.mkdir(models_folder)\n\nfor epoch in range(EPOCHS):\n    \n    print(f\"EPOCH {epoch} started\" + '=' * 30)\n    \n    for idx,paragraph in enumerate(lectures_loader):\n        \n        #################### Fit as many paragraph sequences into MAX_SEQ_LEN sequence as possible ####\n        paragraph_tens = torch.tensor(tokenizer.encode(paragraph[0])).unsqueeze(0).to(device)\n        #Skip sample from dataset if it is longer than MAX_SEQ_LEN\n        if paragraph_tens.size()[1] > MAX_SEQ_LEN:\n            continue\n        \n        #The first parapraph sequence in the sequence\n        if not torch.is_tensor(tmp_paragraphs_tens):\n            tmp_paragraphs_tens = paragraph_tens\n            continue\n        else:\n            #The next paragraph does not fit in so we process the sequence and leave the last paragraph \n            #as the start for next sequence \n            if tmp_paragraphs_tens.size()[1] + paragraph_tens.size()[1] > MAX_SEQ_LEN:\n                work_paragraphs_tens = tmp_paragraphs_tens\n                tmp_paragraphs_tens = paragraph_tens\n            else:\n                #Add the paragraph to sequence, continue and try to add more\n                tmp_paragraphs_tens = torch.cat([tmp_paragraphs_tens, paragraph_tens[:,1:]], dim=1)\n                continue\n        ################## Sequence ready, process it trough the model ##################\n            \n        outputs = model(work_paragraphs_tens, labels=work_paragraphs_tens)\n        loss, logits = outputs[:2]                        \n        loss.backward()\n        sum_loss = sum_loss + loss.detach().data\n                       \n        proc_seq_count += 1\n        if proc_seq_count == BATCH_SIZE:\n            proc_seq_count = 0    \n            batch_count += 1\n            optimizer.step()\n            scheduler.step() \n            optimizer.zero_grad()\n            model.zero_grad()\n\n        if batch_count == 5:\n            print(f\"sum loss is {sum_loss}\")\n            batch_count = 0\n            sum_loss = 0.0\n    \n    # Store the model after each epoch to compare the performance of them\n    torch.save(model.state_dict(), os.path.join(models_folder, f\"gpt2_medium_pythonlecturer_{epoch}.pt\"))\n            ","metadata":{"execution":{"iopub.status.busy":"2022-11-04T20:35:15.944248Z","iopub.execute_input":"2022-11-04T20:35:15.945404Z","iopub.status.idle":"2022-11-04T20:42:57.692291Z","shell.execute_reply.started":"2022-11-04T20:35:15.945368Z","shell.execute_reply":"2022-11-04T20:42:57.690763Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"EPOCH 0 started==============================\nsum loss is 355.9223937988281\nsum loss is 365.61151123046875\nsum loss is 364.03509521484375\nsum loss is 353.56768798828125\nsum loss is 341.7742919921875\nsum loss is 340.7035827636719\nsum loss is 331.640869140625\nsum loss is 309.4134826660156\nsum loss is 322.1365661621094\nsum loss is 323.0478515625\nsum loss is 309.8744812011719\nsum loss is 303.93634033203125\nsum loss is 303.2567138671875\nsum loss is 302.2937316894531\nsum loss is 294.65618896484375\nsum loss is 284.779541015625\nsum loss is 288.5148010253906\nsum loss is 278.5264587402344\nsum loss is 285.6671447753906\nsum loss is 272.9200744628906\nEPOCH 1 started==============================\nsum loss is 288.60601806640625\nsum loss is 275.16339111328125\nsum loss is 263.6115417480469\nsum loss is 279.8761901855469\nsum loss is 281.5300598144531\nsum loss is 277.4534912109375\nsum loss is 266.66168212890625\nsum loss is 265.2763671875\nsum loss is 276.4015808105469\nsum loss is 271.95086669921875\nsum loss is 274.06298828125\nsum loss is 270.006103515625\nsum loss is 267.4972839355469\nsum loss is 275.60943603515625\nsum loss is 265.6352233886719\nsum loss is 262.6449279785156\nsum loss is 265.2775573730469\nsum loss is 260.6457214355469\nsum loss is 270.7906494140625\nsum loss is 263.4117126464844\nEPOCH 2 started==============================\nsum loss is 254.00601196289062\nsum loss is 259.82623291015625\nsum loss is 264.60357666015625\nsum loss is 260.8864440917969\nsum loss is 253.05091857910156\nsum loss is 256.4308166503906\nsum loss is 253.9879608154297\nsum loss is 254.8990478515625\nsum loss is 247.938720703125\nsum loss is 259.202392578125\nsum loss is 251.6371307373047\nsum loss is 246.37318420410156\nsum loss is 244.21571350097656\nsum loss is 251.46444702148438\nsum loss is 246.4121551513672\nsum loss is 252.89804077148438\nsum loss is 257.77655029296875\nsum loss is 257.5509338378906\nsum loss is 248.38320922851562\nsum loss is 255.49037170410156\nEPOCH 3 started==============================\nsum loss is 239.92138671875\nsum loss is 231.57591247558594\nsum loss is 254.2370147705078\nsum loss is 239.8502960205078\nsum loss is 228.23602294921875\nsum loss is 233.94012451171875\nsum loss is 228.8432159423828\nsum loss is 235.37066650390625\nsum loss is 236.53097534179688\nsum loss is 238.48110961914062\nsum loss is 240.60113525390625\nsum loss is 237.06141662597656\nsum loss is 230.34474182128906\nsum loss is 231.50238037109375\nsum loss is 239.68588256835938\nsum loss is 244.37887573242188\nsum loss is 232.2179718017578\nsum loss is 238.1343994140625\nsum loss is 237.66943359375\nsum loss is 243.26063537597656\nEPOCH 4 started==============================\nsum loss is 228.76278686523438\nsum loss is 210.8519287109375\nsum loss is 215.0310821533203\nsum loss is 216.3961639404297\nsum loss is 219.4771270751953\nsum loss is 219.7367706298828\nsum loss is 217.2448272705078\nsum loss is 215.2006072998047\nsum loss is 216.9355926513672\nsum loss is 212.96701049804688\nsum loss is 219.10902404785156\nsum loss is 215.75637817382812\nsum loss is 223.02552795410156\nsum loss is 222.2727508544922\nsum loss is 223.82546997070312\nsum loss is 222.71478271484375\nsum loss is 225.96530151367188\nsum loss is 216.3196563720703\nsum loss is 217.8538818359375\nsum loss is 222.45330810546875\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Generating the lectures","metadata":{}},{"cell_type":"code","source":"def choose_from_top(probs, n=5, random_seed=None):\n    ind = np.argpartition(probs, -n)[-n:]\n    top_prob = probs[ind]\n    top_prob = top_prob / np.sum(top_prob) # Normalize\n    np.random.seed(random_seed)\n    choice = np.random.choice(n, 1, p = top_prob)\n    token_id = ind[choice][0]\n    return int(token_id)","metadata":{"execution":{"iopub.status.busy":"2022-11-04T20:42:57.699031Z","iopub.execute_input":"2022-11-04T20:42:57.702199Z","iopub.status.idle":"2022-11-04T20:42:57.714441Z","shell.execute_reply.started":"2022-11-04T20:42:57.702144Z","shell.execute_reply":"2022-11-04T20:42:57.713049Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"#load the fine-tuned model\nMODEL_EPOCH = 4\nmodel_path = os.path.join(models_folder, f\"gpt2_medium_pythonlecturer_{MODEL_EPOCH}.pt\")\nmodel.load_state_dict(torch.load(model_path))\n\n#set num of paragraphs to generate\ngenerated_paragraph = 3\nfirst_sentence = \"Hello everyone\"\nmax_paragraph_length = 500\n\n#setting random seed \nrandomness = None #default: None, change this to a value for debugging purpose\n\nparagraphs_output_file_path = os.path.join(\"/kaggle/working/\", f'generated_lectures_{MODEL_EPOCH}.txt')\naudio_file_folder = \"/kaggle/working/\"\nif os.path.exists(paragraphs_output_file_path):\n    os.remove(paragraphs_output_file_path)\n\nmodel.eval()\nwith torch.no_grad():\n    for paragraph_idx in range(generated_paragraph):\n        paragraph_finished = False\n        cur_ids = torch.tensor(tokenizer.encode(first_sentence)).unsqueeze(0).to(device)\n\n        for i in range(max_paragraph_length):\n            outputs = model(cur_ids, labels=cur_ids)\n            loss, logits = outputs[:2]\n            softmax_logits = torch.softmax(logits[0,-1], dim=0) #Take the first(from only one in this case) batch and the last predicted embedding\n            if i < 3:\n                n = 20\n            else:\n                n = 3\n                \n            next_token_id = choose_from_top(softmax_logits.to('cpu').numpy(), n=n, random_seed=randomness) #Randomly(from the topN probability distribution) select the next word\n#             print(next_token_id)\n\n            cur_ids = torch.cat([cur_ids, torch.ones((1,1)).long().to(device) * next_token_id], dim = 1) # Add the last word to the running sequence\n\n            if next_token_id in tokenizer.encode('<|endoftext|>'):\n                paragraph_finished = True\n                break\n\n        if paragraph_finished:\n            output_list = list(cur_ids.squeeze().to('cpu').numpy())\n            output_text = tokenizer.decode(output_list)[:-13] #not include '<|endoftext|>'\n            print(output_text + \"\\n\")\n\n            with open(paragraphs_output_file_path, 'a') as f:\n                f.write(f\"{output_text} \\n\\n\")\n                ","metadata":{"execution":{"iopub.status.busy":"2022-11-04T20:42:57.715975Z","iopub.execute_input":"2022-11-04T20:42:57.716350Z","iopub.status.idle":"2022-11-04T20:43:02.445050Z","shell.execute_reply.started":"2022-11-04T20:42:57.716315Z","shell.execute_reply":"2022-11-04T20:43:02.443867Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"Hello everyone, I'm back from my trip to the beach. I'm back to normal.\n\nHello everyone, welcome aboard the new and improved Reddit!\n\nHello everyone. My name is Michael. I'm a musician and I make music for my family. My mom's a musician.\n\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}