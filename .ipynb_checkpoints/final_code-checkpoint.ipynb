{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning GPT-2 on a dataset in PyTorch\n",
    "\n",
    "Fine-tune a pre-trained GPT-2 model on a dataset containing the scripts of a python course. Let's see if the model can learn to teach Python!\n",
    "\n",
    "For this experiment, we will use a pre-trained GPT-2 medium-sized model from the huggingface [transformers repository](https://github.com/huggingface/transformers).\n",
    "\n",
    "This project is developed referencing https://gist.github.com/mf1024/3df214d2f17f3dcc56450ddf0d5a4cd7\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-24T14:57:50.335872Z",
     "iopub.status.busy": "2022-11-24T14:57:50.334667Z",
     "iopub.status.idle": "2022-11-24T14:58:02.359597Z",
     "shell.execute_reply": "2022-11-24T14:58:02.358270Z",
     "shell.execute_reply.started": "2022-11-24T14:57:50.335756Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /opt/conda/lib/python3.7/site-packages (4.20.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.7/site-packages (from transformers) (2021.11.10)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.7/site-packages (from transformers) (1.21.6)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /opt/conda/lib/python3.7/site-packages (from transformers) (0.12.1)\n",
      "Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from transformers) (4.13.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.7/site-packages (from transformers) (4.64.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.7/site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from transformers) (3.7.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /opt/conda/lib/python3.7/site-packages (from transformers) (0.10.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.7/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from transformers) (2.28.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.7/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.1.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging>=20.0->transformers) (3.0.9)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->transformers) (3.8.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (1.26.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (3.3)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (2022.9.24)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install transformers #install transformers from HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-24T14:31:49.612833Z",
     "iopub.status.busy": "2022-11-24T14:31:49.612431Z",
     "iopub.status.idle": "2022-11-24T14:31:51.215362Z",
     "shell.execute_reply": "2022-11-24T14:31:51.214304Z",
     "shell.execute_reply.started": "2022-11-24T14:31:49.612788Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel \n",
    "import numpy as np\n",
    "\n",
    "#GPT2LMHeadModel: it is anguage model, which is GPT2Model, with an additional linear layer \n",
    "#that uses input embedding layer weights to do the inverse operation of the embedding layer - \n",
    "#to create logits vector for the dictionary from outputs of the GPT2.\n",
    "\n",
    "#GPT2Tokenizer: is a byte-code pair encoder that will transform input text input into input tokens \n",
    "#that the huggingface transformers were trained on.\n",
    "\n",
    "import logging\n",
    "logging.getLogger().setLevel(logging.CRITICAL)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "device = 'cpu'\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-24T14:31:51.217043Z",
     "iopub.status.busy": "2022-11-24T14:31:51.216574Z"
    }
   },
   "outputs": [],
   "source": [
    "#load the model and its tokenizer \n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2-medium') #from_pretrained loads the trained model weights\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2-medium')\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use the dataset to create a list of datapoints\n",
    "\n",
    "We will use the dataset of lecture scripts. After each paragraph, we add \"<|endofext|>\" which is recognized by the GPT2 model as the end of text marker. The marker will allow us to concatenate many paragraph in a single input sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os\n",
    "import json\n",
    "import csv\n",
    "\n",
    "class LecturesDataset(Dataset):\n",
    "    def __init__(self, lecture_dataset_path = '/kaggle/input/d/apurvapatil871/pythondata'):\n",
    "        super().__init__() #initialize the Dataset class\n",
    "\n",
    "        lecture_path = os.path.join(lecture_dataset_path, 'trainingdata.csv')\n",
    "\n",
    "        self.lecture_list = []\n",
    "        self.end_of_text_token = \"<|endoftext|>\"\n",
    "        \n",
    "        with open(lecture_path) as csv_file:\n",
    "            csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "        \n",
    "            for row in csv_reader:\n",
    "                lecture_str = f\"{row[1]}{self.end_of_text_token}\"\n",
    "                self.lecture_list.append(lecture_str)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.lecture_list)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        return self.lecture_list[item]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = LecturesDataset()\n",
    "lectures_loader = DataLoader(dataset, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters\n",
    "\n",
    "We mostly tuned ***BATCH_SIZE***, ***EOPOCHS***, and ***LEARNING_RATE***."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16\n",
    "EPOCHS = 5\n",
    "LEARNING_RATE = 3e-5\n",
    "WARMUP_STEPS = 500\n",
    "MAX_SEQ_LEN = 40"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model training\n",
    "\n",
    "We will train the model and save the model weights after each epoch and then we will try to generate python answers with each version of the weight to see which performs the best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "model.train() #put the model in the training mode\n",
    "optimizer = AdamW(model.parameters(), lr=LEARNING_RATE) #select the optimizer\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=WARMUP_STEPS, num_training_steps = -1)\n",
    "proc_seq_count = 0\n",
    "sum_loss = 0.0\n",
    "batch_count = 0\n",
    "\n",
    "tmp_paragraphs_tens = None\n",
    "models_folder = \"/kaggle/working/trained_models\" #folder for the trained model\n",
    "\n",
    "if not os.path.exists(models_folder):\n",
    "    os.mkdir(models_folder)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    \n",
    "    print(f\"EPOCH {epoch} started\" + '=' * 30)\n",
    "    \n",
    "    for idx,paragraph in enumerate(lectures_loader):\n",
    "        \n",
    "        #################### Make a sequence of MAX_SEQ_LEN tokens ####\n",
    "        paragraph_tens = torch.tensor(tokenizer.encode(paragraph[0])).unsqueeze(0).to(device)\n",
    "        #Skip sample from dataset if it is longer than MAX_SEQ_LEN\n",
    "        if paragraph_tens.size()[1] > MAX_SEQ_LEN:\n",
    "            continue\n",
    "        \n",
    "        #The first parapraph sequence in the sequence\n",
    "        if not torch.is_tensor(tmp_paragraphs_tens):\n",
    "            tmp_paragraphs_tens = paragraph_tens\n",
    "            continue\n",
    "        else:\n",
    "            #The next paragraph does not fit in so we process the sequence and leave the last paragraph \n",
    "            #as the start for next sequence \n",
    "            if tmp_paragraphs_tens.size()[1] + paragraph_tens.size()[1] > MAX_SEQ_LEN:\n",
    "                work_paragraphs_tens = tmp_paragraphs_tens\n",
    "                tmp_paragraphs_tens = paragraph_tens\n",
    "            else:\n",
    "                #Add the paragraph to sequence, continue and try to add more\n",
    "                tmp_paragraphs_tens = torch.cat([tmp_paragraphs_tens, paragraph_tens[:,1:]], dim=1)\n",
    "                continue\n",
    "        ################## Sequence ready, process it trough the model ##################\n",
    "            \n",
    "        outputs = model(work_paragraphs_tens, labels=work_paragraphs_tens) #model prediction\n",
    "        loss, logits = outputs[:2] #extract the loss and the output seperately               \n",
    "        loss.backward() #compute the gradients\n",
    "        sum_loss = sum_loss + loss.detach().data\n",
    "                       \n",
    "        proc_seq_count += 1\n",
    "        if proc_seq_count == BATCH_SIZE:\n",
    "            proc_seq_count = 0    \n",
    "            batch_count += 1\n",
    "            optimizer.step() #use the gradients to adjust the parameters\n",
    "            scheduler.step() \n",
    "            optimizer.zero_grad() #reset the optmizer gradients\n",
    "            model.zero_grad() \n",
    "\n",
    "        if batch_count == 100: #for every 100 batches print the loss\n",
    "            print(f\"sum loss is {sum_loss}\")\n",
    "            batch_count = 0\n",
    "            sum_loss = 0.0\n",
    "    \n",
    "    # Store the model after each epoch to compare the performance of them\n",
    "    torch.save(model.state_dict(), os.path.join(models_folder, f\"gpt2_medium_pythonlecturer_{epoch}.pt\"))\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating the answers to input questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to first select topN tokens from the probability list and then based on the selected N word distribution\n",
    "# get random token ID\n",
    "def choose_from_top(probs, n=2, random_seed=None):\n",
    "    ind = np.argpartition(probs, -n)[-n:]\n",
    "    top_prob = probs[ind]\n",
    "    top_prob = top_prob / np.sum(top_prob) # Normalize\n",
    "    np.random.seed(random_seed)\n",
    "    choice = np.random.choice(n, 1, p = top_prob)\n",
    "    token_id = ind[choice][0]\n",
    "    return int(token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the fine-tuned model\n",
    "MODEL_EPOCH = 4\n",
    "model_path = os.path.join(models_folder, f\"gpt2_medium_pythonlecturer_{MODEL_EPOCH}.pt\")\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "\n",
    "#set num of paragraphs to generate\n",
    "generated_paragraph = 5\n",
    "first_sentence = \"Q: What is python? \\n A:\"\n",
    "max_paragraph_length = 100\n",
    "\n",
    "#setting random seed \n",
    "randomness = None #default: None, change this to a value for debugging purpose\n",
    "\n",
    "paragraphs_output_file_path = os.path.join(\"/kaggle/working/\", f'generated_lectures_{MODEL_EPOCH}.txt')\n",
    "audio_file_folder = \"/kaggle/working/\"\n",
    "if os.path.exists(paragraphs_output_file_path):\n",
    "    os.remove(paragraphs_output_file_path)\n",
    "\n",
    "model.eval() #put the model in the evaluation mode\n",
    "with torch.no_grad():\n",
    "    for paragraph_idx in range(generated_paragraph):\n",
    "        paragraph_finished = False\n",
    "        cur_ids = torch.tensor(tokenizer.encode(first_sentence)).unsqueeze(0).to(device) #tokenize the input string and store it as a tensor\n",
    "\n",
    "        for i in range(max_paragraph_length):\n",
    "            outputs = model(cur_ids, labels=cur_ids) #generate the output\n",
    "            loss, logits = outputs[:2]\n",
    "            softmax_logits = torch.softmax(logits[0,-1], dim=0) #Take the first(from only one in this case) batch and the last predicted embedding\n",
    "            if i < 3:\n",
    "                n = 20\n",
    "            else:\n",
    "                n = 3\n",
    "                \n",
    "            next_token_id = choose_from_top(softmax_logits.to('cpu').numpy(), n=n, random_seed=randomness) #Randomly(from the topN probability distribution) select the next word\n",
    "#             print(next_token_id)\n",
    "\n",
    "            cur_ids = torch.cat([cur_ids, torch.ones((1,1)).long().to(device) * next_token_id], dim = 1) # Add the last word to the running sequence\n",
    "\n",
    "            if next_token_id in tokenizer.encode('<|endoftext|>'):\n",
    "                paragraph_finished = True\n",
    "                break\n",
    "\n",
    "        if paragraph_finished:\n",
    "            output_list = list(cur_ids.squeeze().to('cpu').numpy())\n",
    "            output_text = tokenizer.decode(output_list)[:-13] #not include '<|endoftext|>'\n",
    "            print(output_text + \"\\n\")\n",
    "\n",
    "            with open(paragraphs_output_file_path, 'a') as f:\n",
    "                f.write(f\"{output_text} \\n\\n\")\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
