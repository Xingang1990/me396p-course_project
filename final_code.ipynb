{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Fine-tuning GPT-2 on a dataset in PyTorch\n\nFine-tune a pre-trained GPT-2 model on a dataset containing the scripts of a python course. Let's see if the model can learn to teach Python!\n\nFor this experiment, we will use a pre-trained GPT-2 medium-sized model from the huggingface [transformers repository](https://github.com/huggingface/transformers).\n\nThis project is developed referencing https://gist.github.com/mf1024/3df214d2f17f3dcc56450ddf0d5a4cd7\n","metadata":{}},{"cell_type":"code","source":"!pip install transformers","metadata":{"execution":{"iopub.status.busy":"2022-11-24T14:57:50.334667Z","iopub.execute_input":"2022-11-24T14:57:50.335872Z","iopub.status.idle":"2022-11-24T14:58:02.359597Z","shell.execute_reply.started":"2022-11-24T14:57:50.335756Z","shell.execute_reply":"2022-11-24T14:58:02.358270Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /opt/conda/lib/python3.7/site-packages (4.20.1)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.7/site-packages (from transformers) (2021.11.10)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.7/site-packages (from transformers) (1.21.6)\nRequirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /opt/conda/lib/python3.7/site-packages (from transformers) (0.12.1)\nRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from transformers) (4.13.0)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.7/site-packages (from transformers) (4.64.0)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.7/site-packages (from transformers) (21.3)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from transformers) (3.7.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /opt/conda/lib/python3.7/site-packages (from transformers) (0.10.1)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.7/site-packages (from transformers) (6.0)\nRequirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from transformers) (2.28.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.7/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.1.1)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging>=20.0->transformers) (3.0.9)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->transformers) (3.8.0)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (1.26.12)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (3.3)\nRequirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (2.1.0)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (2022.9.24)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nfrom transformers import GPT2Tokenizer, GPT2LMHeadModel\nimport numpy as np\n\nimport logging\nlogging.getLogger().setLevel(logging.CRITICAL)\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\ndevice = 'cpu'\nif torch.cuda.is_available():\n    device = 'cuda'\nprint(device)","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2022-11-24T14:31:49.612431Z","iopub.execute_input":"2022-11-24T14:31:49.612833Z","iopub.status.idle":"2022-11-24T14:31:51.215362Z","shell.execute_reply.started":"2022-11-24T14:31:49.612788Z","shell.execute_reply":"2022-11-24T14:31:51.214304Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer = GPT2Tokenizer.from_pretrained('gpt2-medium')\nmodel = GPT2LMHeadModel.from_pretrained('gpt2-medium')\nmodel = model.to(device)","metadata":{"execution":{"iopub.status.busy":"2022-11-24T14:31:51.216574Z","iopub.execute_input":"2022-11-24T14:31:51.217043Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### PyTorch Dataset module for the dataset\n\nWe will use the dataset of lecture scripts. After each paragraph, we add \"<|endofext|>\" which is recognized by the GPT2 model as the end of text marker. The marker will allow us to concatenate many paragraph in a single input sequence.","metadata":{}},{"cell_type":"code","source":"from torch.utils.data import Dataset\nfrom torch.utils.data import Dataset, DataLoader\nimport os\nimport json\nimport csv\n\nclass LecturesDataset(Dataset):\n    def __init__(self, lecture_dataset_path = '/kaggle/input/d/apurvapatil871/pythondata'):\n        super().__init__() #initialize the Data\n\n        lecture_path = os.path.join(lecture_dataset_path, 'trainingdata.csv')\n\n        self.joke_list = []\n        self.end_of_text_token = \"<|endoftext|>\"\n        \n        with open(lecture_path) as csv_file:\n            csv_reader = csv.reader(csv_file, delimiter=',')\n        \n            for row in csv_reader:\n                lecture_str = f\"data:{row[1]}{self.end_of_text_token}\"\n                self.joke_list.append(lecture_str)\n        \n    def __len__(self):\n        return len(self.joke_list)\n\n    def __getitem__(self, item):\n        return self.joke_list[item]\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset = LecturesDataset()\nlectures_loader = DataLoader(dataset, batch_size=1, shuffle=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Hyperparameters\n\nWe mostly tuned ***BATCH_SIZE***, ***EOPOCHS***, and ***LEARNING_RATE***.","metadata":{}},{"cell_type":"code","source":"BATCH_SIZE = 16\nEPOCHS = 5\nLEARNING_RATE = 3e-5\nWARMUP_STEPS = 500\nMAX_SEQ_LEN = 40","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Model training\n\nWe will train the model and save the model weights after each epoch and then we will try to generate python answers with each version of the weight to see which performs the best.","metadata":{}},{"cell_type":"code","source":"from transformers import AdamW, get_linear_schedule_with_warmup\n\nmodel.train()\noptimizer = AdamW(model.parameters(), lr=LEARNING_RATE)\nscheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=WARMUP_STEPS, num_training_steps = -1)\nproc_seq_count = 0\nsum_loss = 0.0\nbatch_count = 0\n\ntmp_paragraphs_tens = None\nmodels_folder = \"/kaggle/working/trained_models\" #fold for the trained model\n\nif not os.path.exists(models_folder):\n    os.mkdir(models_folder)\n\nfor epoch in range(EPOCHS):\n    \n    print(f\"EPOCH {epoch} started\" + '=' * 30)\n    \n    for idx,paragraph in enumerate(lectures_loader):\n        \n        #################### Fit as many paragraph sequences into MAX_SEQ_LEN sequence as possible ####\n        paragraph_tens = torch.tensor(tokenizer.encode(paragraph[0])).unsqueeze(0).to(device)\n        #Skip sample from dataset if it is longer than MAX_SEQ_LEN\n        if paragraph_tens.size()[1] > MAX_SEQ_LEN:\n            continue\n        \n        #The first parapraph sequence in the sequence\n        if not torch.is_tensor(tmp_paragraphs_tens):\n            tmp_paragraphs_tens = paragraph_tens\n            continue\n        else:\n            #The next paragraph does not fit in so we process the sequence and leave the last paragraph \n            #as the start for next sequence \n            if tmp_paragraphs_tens.size()[1] + paragraph_tens.size()[1] > MAX_SEQ_LEN:\n                work_paragraphs_tens = tmp_paragraphs_tens\n                tmp_paragraphs_tens = paragraph_tens\n            else:\n                #Add the paragraph to sequence, continue and try to add more\n                tmp_paragraphs_tens = torch.cat([tmp_paragraphs_tens, paragraph_tens[:,1:]], dim=1)\n                continue\n        ################## Sequence ready, process it trough the model ##################\n            \n        outputs = model(work_paragraphs_tens, labels=work_paragraphs_tens)\n        loss, logits = outputs[:2]                        \n        loss.backward()\n        sum_loss = sum_loss + loss.detach().data\n                       \n        proc_seq_count += 1\n        if proc_seq_count == BATCH_SIZE:\n            proc_seq_count = 0    \n            batch_count += 1\n            optimizer.step()\n            scheduler.step() \n            optimizer.zero_grad()\n            model.zero_grad()\n\n        if batch_count == 100:\n            print(f\"sum loss is {sum_loss}\")\n            batch_count = 0\n            sum_loss = 0.0\n    \n    # Store the model after each epoch to compare the performance of them\n    torch.save(model.state_dict(), os.path.join(models_folder, f\"gpt2_medium_pythonlecturer_{epoch}.pt\"))\n            ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Generating the lectures","metadata":{}},{"cell_type":"code","source":"def choose_from_top(probs, n=2, random_seed=None):\n    ind = np.argpartition(probs, -n)[-n:]\n    top_prob = probs[ind]\n    top_prob = top_prob / np.sum(top_prob) # Normalize\n    np.random.seed(random_seed)\n    choice = np.random.choice(n, 1, p = top_prob)\n    token_id = ind[choice][0]\n    return int(token_id)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#load the fine-tuned model\nMODEL_EPOCH = 4\nmodel_path = os.path.join(models_folder, f\"gpt2_medium_pythonlecturer_{MODEL_EPOCH}.pt\")\nmodel.load_state_dict(torch.load(model_path))\n\n#set num of paragraphs to generate\ngenerated_paragraph = 5\nfirst_sentence = \"Q: What is python? \\n A:\"\nmax_paragraph_length = 100\n\n#setting random seed \nrandomness = None #default: None, change this to a value for debugging purpose\n\nparagraphs_output_file_path = os.path.join(\"/kaggle/working/\", f'generated_lectures_{MODEL_EPOCH}.txt')\naudio_file_folder = \"/kaggle/working/\"\nif os.path.exists(paragraphs_output_file_path):\n    os.remove(paragraphs_output_file_path)\n\nmodel.eval()\nwith torch.no_grad():\n    for paragraph_idx in range(generated_paragraph):\n        paragraph_finished = False\n        cur_ids = torch.tensor(tokenizer.encode(first_sentence)).unsqueeze(0).to(device)\n\n        for i in range(max_paragraph_length):\n            outputs = model(cur_ids, labels=cur_ids)\n            loss, logits = outputs[:2]\n            softmax_logits = torch.softmax(logits[0,-1], dim=0) #Take the first(from only one in this case) batch and the last predicted embedding\n            if i < 3:\n                n = 20\n            else:\n                n = 3\n                \n            next_token_id = choose_from_top(softmax_logits.to('cpu').numpy(), n=n, random_seed=randomness) #Randomly(from the topN probability distribution) select the next word\n#             print(next_token_id)\n\n            cur_ids = torch.cat([cur_ids, torch.ones((1,1)).long().to(device) * next_token_id], dim = 1) # Add the last word to the running sequence\n\n            if next_token_id in tokenizer.encode('<|endoftext|>'):\n                paragraph_finished = True\n                break\n\n        if paragraph_finished:\n            output_list = list(cur_ids.squeeze().to('cpu').numpy())\n            output_text = tokenizer.decode(output_list)[:-13] #not include '<|endoftext|>'\n            output_text = tokenizer.decode(output_list)\n            print(output_text + \"\\n\")\n\n            with open(paragraphs_output_file_path, 'a') as f:\n                f.write(f\"{output_text} \\n\\n\")\n                ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}